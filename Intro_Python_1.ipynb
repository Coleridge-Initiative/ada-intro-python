{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Workbook 1: What are the characteristics in the number of jobs by census block?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these workbooks, we will start with a motivating question, then walk through the process we need to go through in order to answer the motivating question. Along the way, we will walk through various Python commands and develop skills as we work towards answering the question.\n",
    "\n",
    "As you work, there will be headers that are in **<span style=\"color:green\">GREEN</span>**. These indicate locations where there is an accompanying video. This video may walk through the steps or expand on the topics discussed in that section. Though it isn't absolutely necessary to watch the video while working through this notebook, we highly recommend watching them at least once on your first time through.\n",
    "\n",
    "**<span style = \"color:green\">If you have not yet watched the \"Introduction to Jupyter Notebooks\" video, watch it before you proceed!</span>**\n",
    "\n",
    "You will also run into some headers that are in **<span style=\"color:red\">RED</span>**. These headers indicate a checkpoint to practice writing the code yourself. You should stop at these checkpoints and try doing the exercises and answering the questions posed in these sections.\n",
    "\n",
    "**NOTE: When you open a notebook, make sure you run each cell containing code from the beginning. Since the code we're writing builds on everything written before, if you don't make sure to run everything from the beginning, some things may not work.**\n",
    "\n",
    "In each of the workbooks, we will start out with a motivating question. Here, we'll introduce the data that we'll work with, which will lead into our motivating question for this workbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longitudinal Employer-Household Dynamics (LEHD) Data\n",
    "\n",
    "In these workbooks, we will be using LEHD data. These are public-use data sets containing information about employers and employees. Information about the LEHD Data can be found at [https://lehd.ces.census.gov/](https://lehd.ces.census.gov/). \n",
    "\n",
    "We will be using the LEHD Origin-Destination Employment Statistics (LODES) datasets in our applications in this workbook. Each state has three main types of files: Origin-Destination data, in which job totals are associated with a home and work Census block pair, Residence Area Characteristic data, in which job totals are by home Census block, and Workplace Area Characteristic data, in which job totals are by workplace Census block. In addition to these three, there is a \"geographic crosswalk\" file with descriptions of the Census Blocks as they appear the in the LODES datasets.\n",
    "\n",
    "You can find more information about the LODES datasets [here](https://lehd.ces.census.gov/data/lodes/LODES7/LODESTechDoc7.3.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color:green\">Motivating Question (VIDEO)</span>\n",
    "\n",
    "The dataset we'll be working with in this workbook is the Workplace Area Characteristics data, which aggregates job totals by workplace census block. We want to explore this dataset and get a better idea of the distribution of jobs. That is, we want to answer the following questions:\n",
    "\n",
    "**How can we characterize the number of jobs in the state? What can we say about the distribution of the number of jobs by census block? What are distributions of jobs by different categories, such as age group or industry?**\n",
    "\n",
    "As you work through this notebook, we'll work towards answering these questions, so try to keep in mind what we're working toward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Out: Introduction to Python\n",
    "\n",
    "In order to try to answer these questions, we'll write code to bring in datasets, manipulate these datasets, and summarize datasets using Python. Python is a popular general-purpose programming language that has seen a rise in use for data analysis. As of 2017, Python is near or at the top in terms of popularity in programming languages for data analysis (see [the kdnuggets post](https://www.kdnuggets.com/2017/05/poll-analytics-data-science-machine-learning-software-leaders.html) or [a look at multiple surveys](http://makemeanalyst.com/most-popular-languages-for-data-science-and-analytics-2017/)).\n",
    "\n",
    "In addition, unlike other tools for statistical analysis, like Excel or Stata, Python is designed to be general-purpose. This means that we aren't limited to only doing certain statistical analyses and gives us much more flexibility in what we can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries\n",
    "\n",
    "We first load a few libraries. These libraries are essentially bundles of useful tools that can help us do specific tasks. In this case, we're going to be bringing in NumPy and Pandas, which are specifically useful for computing and data analysis. Don't worry too much about the specifics of libraries for now, just that we need to include the first few lines of code below if we want to use many of the tools described in this workbook. \n",
    "\n",
    "If you'd like to read more about the libraries that we're loading here, see the following links: for [NumPy](http://www.numpy.org/) and for [Pandas](http://pandas-docs.github.io/pandas-docs-travis/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we import packages\n",
    "# Note that everything after a \"#\" in Python will be ignored, so you can use it to write comments\n",
    "import numpy as np # NumPy (Numerical Python) for scientific computing\n",
    "import pandas as pd # Pandas, for data analysis tools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we loaded the libraries with aliases (e.g. `as np`). This is just to make it easier to use them later on. Whenever you see `np`, read it as `numpy`. The same goes for `pd` and `pandas`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color:green\">Reading in the Data Set (VIDEO)</span>\n",
    "We'll start by reading in a data set from a csv, or comma-separated value, file. For our examples, we'll use the Workplace Area Characteristic (WAC) data from California. \n",
    "\n",
    "We use the `read_csv` function from pandas to read in the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'ca_wac_S000_JT00_2015.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "# We can also use the following line by itself instead\n",
    "# df = pd.read_csv('ca_wac_S000_JT00_2015.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break the code down. In the first line, we assign `'ca_wac_S000_JT00_2015.csv'` to the variable `data_file`. Note that any text inside quotation marks, such as `'ca_wac_S000_JT00_2015.csv'`, is a string, which makes `data_file` a string variable. Note that this by itself doesn't really do anything fancy. We are just setting up a string variable with the text, `'ca_wac_S000_JT00_2015.csv'`, not telling Python to look for a file with that name or anything like that yet.\n",
    "\n",
    "To look at what we've stored inside `data_file`, try using the `print` function with `data_file` as the argument. What do you think the output will be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second line, we're using the `read_csv()` function from the `pandas` package. Notice that we had to include `pd.` in front of the function. This tells Python to use the `read_csv` function that is inside the `pandas` package. The function `read_csv` outputs a Data Frame, which is then assigned to the variable `df`. This means that our data is now in a data frame called `df`.\n",
    "\n",
    "> #### Side Note: File Location\n",
    "> We only used the file name for `data_file`. This is because we included the CSV file in the same folder as this notebook. If it were somewhere else, we'd have to include the file path (e.g. `\"/Documents/precourse/Python/ca_wac_S000_JT00_2015.csv\"`). If you don't know much about how file paths work, don't worry: you just need to make sure that the file is in the same folder as the notebook.\n",
    "\n",
    "Lastly, we've included a line of code that can load the csv file into `df` in one line. This does the exact same thing as the first two lines of code, with the exception of not assigning `'ca_wac_S000_JT00_2015.csv'` to `data_file`. Notice that all we did was replace `data_file` with the string that we assigned to `data_file`. It is commented out, but feel free to try running it by itself (commenting out the first two lines) to check that it does the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Data Types\n",
    "\n",
    "We've mentioned that `data_file` is a string variable and that `df` is a pandas Data Frame. These are different variable types, and it's important to keep this in mind because the type of variable dictates what you can do with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `data_file` is a string, the `type` function returns `str` (which stands for string). Let's look at `df`. What do you think the output will be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tells us that `df` is a pandas Data Frame. As we'll see later on, a Pandas Data Frame, as with other Python objects, has specific attributes and functions that you can use with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Checkpoint 1: Read in Other Data</span>\n",
    "\n",
    "You can access LODES data from other states by using the link below: \n",
    "\n",
    "[LODES Data](https://lehd.ces.census.gov/data/lodes/LODES7)\n",
    "\n",
    "and navigating to the state you want. Check out the [LODES documentation](https://lehd.ces.census.gov/data/lodes/LODES7/LODESTechDoc7.3.pdf) for more information.\n",
    "\n",
    "If you download any data, it will go to your local computer. However, since this notebook is running from a server in the cloud, you'll have to upload it to this server to access it. You can do so by navigating to the previous tab (**Home**), and clicking the Upload button in right-hand corner.\n",
    "\n",
    "If you have problems with that, don't worry. We've included the Illinois LODES data in this environment too. The file is named `il_wac_S000_JT00_2015.csv`. See if you can load it similarly to how you loaded the California data above.\n",
    "\n",
    "Make sure you assign it to a variable other than `df` so that you don't overwrite the data we loaded earlier (for example, if you choose Illinois, you might use `df_il`). Play around with the few functions you've learned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Exploring the Data Frame (VIDEO)</span>\n",
    "\n",
    "Now that we've loaded in the data set as a Data Frame, let's check the number of rows and columns. We can do this by looking at the `shape` attribute of a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are 243,462 rows and 53 columns.\n",
    "\n",
    "Let's also find out the names of all the variables in this data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `head` and `tail` methods in order to look at the first or last few rows of the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # Default is to show first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10) # We can specify how many rows we want to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10) # Same as head, except the last 10 instead of first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Side Note: Instance variables and Methods\n",
    "Note that we used `head()`, with parentheses, while we used just `shape` or `column`, without parentheses. This is because `shape` and `column` are **instance variables** and head is a **method**. To put it another way, `shape` and `columns` are variables that each Data Frame object has, and we're just displaying the values in those variables. On the other hand, `head` is a method, or a function that you perform specifically on a certain type of object (in this case, a Data Frame object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the Data Frame\n",
    "What if we want to only look at certain cells, or certain columns? We can use a variety of commands to do just that.\n",
    "\n",
    "To access individual columns, we can use square brackets or we can simply use dot notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at just total number of jobs (C000)\n",
    "df[\"C000\"] \n",
    "\n",
    "# This does the same thing\n",
    "df.C000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to get certain rows? We can also use `loc` with square brackets. We use a colon to indicate that we want a series of indices with a start and end. We can also leave one side of the colon empty to indicate that we want the rest of the values on that end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rows 10 - 20. Remember, the first row is row 0\n",
    "df.loc[10:20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:] # This gives all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can use `loc` to access certain columns as well as certain indices in the Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at rows 10 - 20 for total number of jobs (C000)\n",
    "df.loc[10:20,\"C000\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at rows 10 - 20 for total number of jobs (C000) and jobs by age group\n",
    "df.loc[10:20,['C000','CA01','CA02','CA03'] ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we wanted to select 4 variables to look at. Notice that we replaced `\"C000\"` with `['C000','CA01','CA02','CA03']`. The square brackets create a list with 4 elements, `'C000'`,`'CA01'`,`'CA02'`, and `'CA03'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(['C000','CA01','CA02','CA03'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_show = ['CA01','CA02','CA03'] # A list of strings containing names of variables (jobs by age group)\n",
    "df.iloc[-5:][vars_to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we were able to use the \"`-5:`\" to indicate that we want the last 5 rows of the data frame. Note that we can't do the same with `.loc`. This is because `.loc` retrieves the rows from a particular *label* in the Data Frame, while `.iloc` retrieves them from particular *positions*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Creating Subsets of the Data (VIDEO)</span>\n",
    "Let's look at how many census blocks had more than 50 jobs. We can do this by including a conditional statement inside brackets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over50 = df[df['C000'] > 50]\n",
    "df_over50.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how exactly this works, let's see what happens when we run only what's inside the brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['C000'] > 50 # This will be a pandas Series with True and False values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code evaluates whether the value in `C000` is above 50 for each row in the data frame, creating a Series with length equal to the number of rows with \"True\" or \"False\" depending on whether that row had a value above 50 in `C000`. By putting this conditional statement into the brackets, we are telling Python which rows we want to include in our subsetted data frame using `True` and `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use more complicated subsets. For example, let's say we wanted to look at the census blocks with between 50 and 100 jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50to100 = df[(df['C000'] > 50) & (df['C000'] < 100)]\n",
    "df_50to100.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we replaced our conditional statement from before with `(df['C000'] > 50) & (df['C000'] < 100)`. This says that we want each row that meets both conditions, `(df['C000'] > 50)` and `(df['C000'] < 100)`. \n",
    "\n",
    "What about the rest? Well, we can find the rows with less than or equal to 50 jobs OR greater than or equal to 100 jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not50to100 = df[(df['C000'] <= 50) | (df['C000'] >= 100)]\n",
    "df_not50to100.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Checkpoint 2: Explore Your Data\n",
    "\n",
    "Now look at the data frame you loaded earlier using the tools we've just covered. Do the number of rows and columns make sense? Try subsetting the data set. How does it compare to the results from California? Does it make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Missing Values\n",
    "Now that we have our data set, let's do a quick check for missing values. This is typically one of the first things you'll want to do when exploring a new data set. \n",
    "\n",
    "Below, we've shown two different ways of writing the same thing. Using `isnull()` gives us a data frame of the same size with `True` and `False` values depending on whether it was a missing value or not. Then, `sum()` sums each column. Since Python treats `True` as `1` and `False` as `0`, the sum of each column gives us the total number of missing values for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null = df.isnull()\n",
    "df_null.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did this in two separate lines, but that's actually not necessary. In fact, we can do it all in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also drop any duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dups = df.drop_duplicates()\n",
    "df_no_dups.shape # Check how many rows there are after dropping duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color:green\">Checking for Inconsistencies (VIDEO)</span>\n",
    "\n",
    "If you check the data documentation, you'll see that `C000` is the total number of jobs. Therefore, it would make sense for the other groups to columns to add up to the values in `C000`. For example, you'd expect `CA01`, `CA02`, and `CA03` to add up to `C000` for each row. Let's check to see if this is true.\n",
    "\n",
    "We'll first take the sum of `CA01`, `CA02`, and `CA03` in each row and put that in a new column called `CA_sum`. Then, we'll compare our new `CA_sum` column to the existing `C000` column to see if they match. We'll first show all the code, then explain each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the columns we want to add up\n",
    "vars_to_check = ['CA01','CA02','CA03']\n",
    "\n",
    "# Using apply to sum the columns for each row\n",
    "df['CA_sum'] = df[vars_to_check].apply(sum,1)\n",
    "\n",
    "# Check how many rows don't match\n",
    "sum(df.CA_sum != df.C000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first created a list called `vars_to_check`, which contains the columns that we want to add up. Then, we took those columns from `df` and used the `apply()` method, which applies the same function to each row (or column, if we used `0` in the second argument instead of `1`). In this case, we want to find the sum of each row, so the first argument is `sum`. We want to create a new column that contains this sum, so we assign that to a new column in `df`, `CA_sum`. Notice that this is the first place we see `'CA_sum'`, because this is where we are creating it. \n",
    "\n",
    "Lastly, we want to check how many rows in which `C000` and `CA_sum` differ. We do this by using\n",
    "\n",
    "    df.CA_sum != df.C000\n",
    "\n",
    "which outputs a Series of `True` and `False` values: `True` if the value in `CA_sum` is not equal to the value in `C000` for that row, and `False` otherwise. In other words, this is a Series of `True`s and `False`s indicating whether the values for the row didn't match. We can then use the `sum` function from NumPy to add up how many times they didn't match. If there are no errors, the sum should be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Outliers\n",
    "\n",
    "Suppose we want to check if there are any outliers in total number of jobs by census block. We can sort the values in `C000` in order to figure this out. Let's say we want to find the top ten census blocks by total number of jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"C000\",ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this down piece by piece. First, we use the `sort_values()` method to sort the Data Frame by `C000`. We use `ascending=False` so that the highest values are at the top (the default is to sort in ascending order). This would give us \n",
    "\n",
    "    df.sort_values(\"C000\",ascending=False)\n",
    "\n",
    "However, we don't want to look at everything. Here, we use `head()` to give us only the top ten values after sorting. This gives us the final code, `df.sort_values(\"C000\",ascending=False).head(10)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "Let's try to get some useful summary statistics of the variables in the data set. We use `describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this gives us the count, mean, standard deviation, minimum, 25th percentile (first quartile), 50th percentile (median), 75th percentile (third quartile), and maximum. Notice that it gives us these summaries even for the variables that don't make sense (the geocode variables). \n",
    "\n",
    "This gives a lot more information than you might want. What if we just wanted to look at a few columns? You might do something like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"C000\",\"CA01\",\"CA02\",\"CA03\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also decide that you only want to find certain values, like the mean. You can do that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"C000\",\"CA01\",\"CA02\",\"CA03\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Descriptive Statistics\n",
    "\n",
    "Before we answer the question posed at the beginning of the workbook, we want to do a little more exploration of the data set. Many times, this is where we might find out how exactly the question should be framed. For example, we might want to know the distribution of jobs by age group for blocks with greater than 50 jobs. How would we do this?\n",
    "\n",
    "Recall that we made a subset of the `df` Data Frame earlier called `df_over50`. We can use `describe()` on this subsetted Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over50.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create new variables. Here, we're going to create an indicator for whether a census block has more than 50 jobs in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"over50\"] = df.C000 > 50\n",
    "df[[\"C000\",\"over50\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't have a variable in `df` called `over50` before we run the first line in the above cell. What we're doing with that one line is creating a new column called `over50` and filling it with the values in the conditional statement `df.C000 > 50`. We've then printed out the first few rows using `head()` to see that it's working properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Checkpoint 3: Descriptive Statistics on Your Data</span>\n",
    "\n",
    "Using the tools described above, look at the data you loaded in earlier. Make sure you know the answers to each of the following questions:\n",
    "- Are there any missing values?\n",
    "- What is the mean of each variable?\n",
    "- Are there any inconsistencies in the data? \n",
    "- Are there missing values that may not have been coded as missing?\n",
    "- Are there any interesting outliers?\n",
    "\n",
    "In addition, try to think about the distribution of jobs by different characteristics like age group and industry. Which age group had the most jobs in the state? Which industry?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
